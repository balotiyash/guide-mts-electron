{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbd9d4f",
   "metadata": {},
   "source": [
    "Removed duplicate constrain from ll1 ll2 mdl and vehicle no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcad19a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.43-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.4-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.1/2.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 9.7 MB/s  0:00:00\n",
      "Downloading greenlet-3.2.4-cp313-cp313-win_amd64.whl (299 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: typing-extensions, greenlet, sqlalchemy\n",
      "\n",
      "   ------------- -------------------------- 1/3 [greenlet]\n",
      "   ------------- -------------------------- 1/3 [greenlet]\n",
      "   ------------- -------------------------- 1/3 [greenlet]\n",
      "   ------------- -------------------------- 1/3 [greenlet]\n",
      "   ------------- -------------------------- 1/3 [greenlet]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   -------------------------- ------------- 2/3 [sqlalchemy]\n",
      "   ---------------------------------------- 3/3 [sqlalchemy]\n",
      "\n",
      "Successfully installed greenlet-3.2.4 sqlalchemy-2.0.43 typing-extensions-4.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb960926",
   "metadata": {},
   "source": [
    "Below is for keeping only 1st entry for duplicate record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f2901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balot\\AppData\\Local\\Temp\\ipykernel_18108\\2048895274.py:61: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  yash_students[col] = pd.to_datetime(yash_students[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data migrated with upsert (insert/update) successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# --- Load CSVs ---\n",
    "mst_jobs = pd.read_csv(\"./29092025_Latest_DB/csv/mst_job.csv\")\n",
    "tbl_learnlicdet = pd.read_csv(\"./29092025_Latest_DB/csv/tbl_learnlicdet.csv\")\n",
    "tbl_permanentlicdet = pd.read_csv(\"./29092025_Latest_DB/csv/tbl_permanentlicdet.csv\")\n",
    "tbl_stufees = pd.read_csv(\"./29092025_Latest_DB/csv/tbl_stufees.csv\")\n",
    "yash_students = pd.read_csv(\"./29092025_Latest_DB/csv/yash_students.csv\")\n",
    "\n",
    "# --- Lowercase text values only ---\n",
    "def lowercase_df(df):\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].str.lower().str.strip()\n",
    "    return df\n",
    "\n",
    "yash_students = lowercase_df(yash_students)\n",
    "\n",
    "# --- Transform columns ---\n",
    "yash_students[['ll_no_1', 'll_no_2']] = yash_students['learn_lic_no'].str.split(pat=',', n=1, expand=True)\n",
    "yash_students[['ll_class_1', 'll_class_2']] = yash_students['learn_lic_type'].str.split(pat=',', n=1, expand=True)\n",
    "\n",
    "yash_students.rename(columns={\n",
    "    \"StuID\": \"id\",\n",
    "    \"StuPhone\": \"mobile_number\",\n",
    "    \"StuName\": \"customer_name\",\n",
    "    \"StuDOB\": \"customer_dob\",\n",
    "    \"StuPerAdd\": \"address\",\n",
    "    \"CarID\": \"vehicle_id\",\n",
    "    \"InstID\": \"instructor_id\",\n",
    "    \"learn_dt_issue\": \"ll_issued_date\",\n",
    "    \"learn_dt_expiry\": \"ll_validity_date\",\n",
    "    \"perm_lic_no\": \"mdl_no\",\n",
    "    \"perm_lic_type\": \"mdl_class\",\n",
    "    \"perm_dt_issue\": \"mdl_issued_date\",\n",
    "    \"perm_dt_expiry\": \"mdl_validity_date\",\n",
    "    \"Endorse\": \"endorsement\",\n",
    "    \"EndorseDate\": \"endorsement_date\",\n",
    "    \"StuSDWOf\": \"relation_name\",\n",
    "    \"LastUpdatedDate\": \"created_on\"\n",
    "}, inplace=True)\n",
    "\n",
    "# --- Add missing DB columns ---\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "yash_students[\"updated_on\"] = now\n",
    "yash_students[\"customer_image\"] = None\n",
    "yash_students[\"customer_signature\"] = None\n",
    "yash_students[\"endorsement_validity_date\"] = None\n",
    "yash_students[\"customer_vehicle_no\"] = None\n",
    "\n",
    "# --- Fix dates (invalid → None, not \"NaT\") ---\n",
    "date_cols = [\n",
    "    'customer_dob', 'll_issued_date', 'll_validity_date',\n",
    "    'mdl_issued_date', 'mdl_validity_date',\n",
    "    'endorsement_date', 'created_on'\n",
    "]\n",
    "for col in date_cols:\n",
    "    if col in yash_students.columns:\n",
    "        yash_students[col] = pd.to_datetime(yash_students[col], errors='coerce')\n",
    "        yash_students[col] = yash_students[col].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        yash_students[col] = yash_students[col].where(pd.notnull(yash_students[col]), None)\n",
    "\n",
    "# --- Insert/Update into SQLite ---\n",
    "database = \"./29092025_Latest_DB/guide-mts-database.sqlite3\"\n",
    "engine = create_engine(f\"sqlite:///{database}\")\n",
    "\n",
    "# Get table columns\n",
    "with engine.begin() as conn:\n",
    "    res = conn.execute(text(\"PRAGMA table_info(customers)\")).fetchall()\n",
    "    table_columns = [row[1] for row in res]\n",
    "\n",
    "# Match dataframe to DB columns\n",
    "columns_to_insert = [col for col in table_columns if col in yash_students.columns]\n",
    "df = yash_students[columns_to_insert].copy()\n",
    "\n",
    "# Remove rows with duplicate mobile_number except for the first occurrence\n",
    "df = df.drop_duplicates(subset=['mobile_number'], keep='first')\n",
    "\n",
    "# UPSERT (insert or update on id conflict)\n",
    "with engine.begin() as conn:\n",
    "    for _, row in df.iterrows():\n",
    "        data = row.to_dict()\n",
    "        cols = \", \".join(data.keys())\n",
    "        placeholders = \", \".join([f\":{c}\" for c in data.keys()])\n",
    "        updates = \", \".join([f\"{c}=excluded.{c}\" for c in data.keys() if c != \"id\"])\n",
    "\n",
    "        sql = text(f\"\"\"\n",
    "            INSERT INTO customers ({cols})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT(id) DO UPDATE SET {updates};\n",
    "        \"\"\")\n",
    "        conn.execute(sql, data)\n",
    "\n",
    "print(\"✅ Data migrated with upsert (insert/update) successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1f7e7",
   "metadata": {},
   "source": [
    "below is for merging dupluicate entry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bf4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balot\\AppData\\Local\\Temp\\ipykernel_13564\\731562474.py:62: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  yash_students[col] = pd.to_datetime(yash_students[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data merged successfully with NULL-safe updates!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# --- Load CSVs ---\n",
    "mst_jobs = pd.read_csv(\"./29092025_Latest_DB/csv/mst_job.csv\")\n",
    "tbl_learnlicdet = pd.read_csv(\"./29092025_Latest_DB/csv/tbl_learnlicdet.csv\")\n",
    "tbl_permanentlicdet = pd.read_csv(\"./29092025_Latest_DB/csv/tbl_permanentlicdet.csv\")\n",
    "tbl_stufees = pd.read_csv(\"./29092025_Latest_DB/csv/tbl_stufees.csv\")\n",
    "yash_students = pd.read_csv(\"./29092025_Latest_DB/csv/yash_students.csv\")\n",
    "\n",
    "# --- Lowercase text values only ---\n",
    "def lowercase_df(df):\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].str.lower().str.strip()\n",
    "    return df\n",
    "\n",
    "yash_students = lowercase_df(yash_students)\n",
    "\n",
    "# --- Transform columns ---\n",
    "yash_students[['ll_no_1', 'll_no_2']] = yash_students['learn_lic_no'].str.split(pat=',', n=1, expand=True)\n",
    "yash_students[['ll_class_1', 'll_class_2']] = yash_students['learn_lic_type'].str.split(pat=',', n=1, expand=True)\n",
    "\n",
    "yash_students.rename(columns={\n",
    "    \"StuID\": \"id\",\n",
    "    \"StuPhone\": \"mobile_number\",\n",
    "    \"StuName\": \"customer_name\",\n",
    "    \"StuDOB\": \"customer_dob\",\n",
    "    \"StuPerAdd\": \"address\",\n",
    "    \"CarID\": \"vehicle_id\",\n",
    "    \"InstID\": \"instructor_id\",\n",
    "    \"learn_dt_issue\": \"ll_issued_date\",\n",
    "    \"learn_dt_expiry\": \"ll_validity_date\",\n",
    "    \"perm_lic_no\": \"mdl_no\",\n",
    "    \"perm_lic_type\": \"mdl_class\",\n",
    "    \"perm_dt_issue\": \"mdl_issued_date\",\n",
    "    \"perm_dt_expiry\": \"mdl_validity_date\",\n",
    "    \"Endorse\": \"endorsement\",\n",
    "    \"EndorseDate\": \"endorsement_date\",\n",
    "    \"StuSDWOf\": \"relation_name\",\n",
    "    \"LastUpdatedDate\": \"created_on\"\n",
    "}, inplace=True)\n",
    "\n",
    "# --- Add missing DB columns ---\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "yash_students[\"updated_on\"] = now\n",
    "yash_students[\"customer_image\"] = None\n",
    "yash_students[\"customer_signature\"] = None\n",
    "yash_students[\"endorsement_validity_date\"] = None\n",
    "yash_students[\"customer_vehicle_no\"] = None\n",
    "\n",
    "# --- Fix dates (invalid → None, not \"NaT\") ---\n",
    "date_cols = [\n",
    "    'customer_dob', 'll_issued_date', 'll_validity_date',\n",
    "    'mdl_issued_date', 'mdl_validity_date',\n",
    "    'endorsement_date', 'created_on'\n",
    "]\n",
    "\n",
    "for col in date_cols:\n",
    "    if col in yash_students.columns:\n",
    "        yash_students[col] = pd.to_datetime(yash_students[col], errors='coerce')\n",
    "        yash_students[col] = yash_students[col].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        yash_students[col] = yash_students[col].where(pd.notnull(yash_students[col]), None)\n",
    "\n",
    "# DB path\n",
    "database = \"./29092025_Latest_DB/guide-mts-database.sqlite3\"\n",
    "engine = create_engine(f\"sqlite:///{database}\")\n",
    "\n",
    "# Assume yash_students dataframe already prepared & cleaned as in previous script\n",
    "\n",
    "# Get table columns\n",
    "with engine.begin() as conn:\n",
    "    res = conn.execute(text(\"PRAGMA table_info(customers)\")).fetchall()\n",
    "    table_columns = [row[1] for row in res]\n",
    "\n",
    "columns_to_insert = [col for col in table_columns if col in yash_students.columns]\n",
    "df = yash_students[columns_to_insert].copy()\n",
    "\n",
    "# Build UPSERT with COALESCE (keep old value if new is NULL)\n",
    "with engine.begin() as conn:\n",
    "    for _, row in df.iterrows():\n",
    "        data = row.dropna(how=\"all\").to_dict()   # drop rows with all NaNs/None\n",
    "        if not data:\n",
    "            continue\n",
    "\n",
    "        cols = \", \".join(data.keys())\n",
    "        placeholders = \", \".join([f\":{c}\" for c in data.keys()])\n",
    "\n",
    "        update_cols = [c for c in data.keys() if c != \"mobile_number\"]\n",
    "        if not update_cols:\n",
    "            continue  # nothing to update\n",
    "\n",
    "        updates = \", \".join([\n",
    "            f\"{c} = COALESCE(excluded.{c}, customers.{c})\"\n",
    "            for c in update_cols\n",
    "        ])\n",
    "\n",
    "        sql = text(f\"\"\"\n",
    "            INSERT INTO customers ({cols})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT(mobile_number) DO UPDATE SET {updates};\n",
    "        \"\"\")\n",
    "        conn.execute(sql, data)\n",
    "\n",
    "\n",
    "print(\"✅ Data merged successfully with NULL-safe updates!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
